{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbf411ac-909e-44a6-9587-f268eba59e19",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bec3d3-d782-41f9-aa8a-9f7948530089",
   "metadata": {},
   "source": [
    "## import required pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcc1dc0d-fb5e-4442-9414-236acd27e249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a3cf18-991e-4312-8813-20f262146e69",
   "metadata": {},
   "source": [
    "# load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78ae1eaa-65f1-4ebe-a7e6-750809adbcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.txt\", \"r\") as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7345fcf5-e902-46f9-a358-f99a3a79fbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice was beginning to get very tired of sitting by her sister\\non the bank, and of having nothing to'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38635efd-14cd-49a9-8109-980212db01a4",
   "metadata": {},
   "source": [
    "### create Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c702ec6-e5f0-4b6f-b45c-976444609c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordTokenizer:\n",
    "    def __init__(self, raw_text):\n",
    "\n",
    "        # split the data into words\n",
    "        words = re.split(r'[,.?|!\"\\' ]', raw_text)\n",
    "        \n",
    "        # remove the empty words\n",
    "        words = [word.strip() for word in words if word.strip()]\n",
    "        \n",
    "        # find the unique words and sort them\n",
    "        self.vocab = sorted(list(set(words)))\n",
    "        \n",
    "        # build the dictionaries\n",
    "        self.stoi = {ch:i for i, ch in enumerate(self.vocab)}\n",
    "        self.itos = {i:ch for i, ch in enumerate(self.vocab)}\n",
    "\n",
    "    def encode(self, words):\n",
    "        # convert every character to the correspoding integer (token id) value\n",
    "        return [self.stoi[word] for word in words]\n",
    "\n",
    "    def decode(self, encoded_value):\n",
    "        # convert every integer into corresponding character\n",
    "        return \" \".join([self.itos[num] for num in encoded_value])\n",
    "\n",
    "    def print_info(self):\n",
    "        print(self.stoi)\n",
    "        print(self.itos)\n",
    "\n",
    "    def n_vocab(self):\n",
    "        return len(self.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55438adc-b347-4bb3-8279-54fb2009f7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of vocab = 4677\n"
     ]
    }
   ],
   "source": [
    "# create tokenizer\n",
    "tokenizer = WordTokenizer(raw_text = raw_text)\n",
    "\n",
    "# print the size of vocab\n",
    "print(f\"size of vocab = {tokenizer.n_vocab()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea8296d-bac7-417f-bfab-a791e24156e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the tokens\n",
    "words = re.split(r'[,.?|!\"\\' ]', raw_text)\n",
    "words = [word.strip() for word in words if word.strip()]\n",
    "tokens = tokenizer.encode(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d134b8e-daf1-4ca1-b907-39b7e32fac5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[70, 4436, 1251, 4175, 2074, 4387, 4172, 2968, 3676, 1360]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aad258a-c79f-4384-99c0-63db725a1a79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice was beginning to get very tired of sitting by'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d84cc5-0b79-4adf-9d61-b67a1512ca3f",
   "metadata": {},
   "source": [
    "# Use tiktoken to create the tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c3618b-7b51-49bd-b4ba-3d4711a3f95f",
   "metadata": {},
   "source": [
    "## gpt2 tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0f903ae-2b89-49ae-b15e-ea4048dfa8dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size gpt2= 50257\n"
     ]
    }
   ],
   "source": [
    "# create the tokenizer using gpt2 vocabulary\n",
    "tokenizer = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "# print the vocab size\n",
    "print(f\"vocab size gpt2= {tokenizer.n_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f855dd46-dc3a-4df5-a151-f69e01e510d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[44484, 373, 3726, 284, 651, 845, 10032, 286, 5586, 416]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = tokenizer.encode(raw_text)\n",
    "token[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "999af4e9-c2da-463f-a801-99764435ce9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice was beginning to get very tired of sitting by'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(token[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a9c9ef-86c1-488e-879d-86bdf2062dc2",
   "metadata": {},
   "source": [
    "## gpt-4o tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0beef414-32ca-401e-8bc3-9b2b6b346356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vocab size of gpt-4o = 200019\n"
     ]
    }
   ],
   "source": [
    "tokenizer2 = tiktoken.encoding_for_model(\"gpt-4o\")\n",
    "print(f\" vocab size of gpt-4o = {tokenizer2.n_vocab}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52c83ffc-7376-44ec-b4c3-05a3ec503b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100151, 673, 10526, 316, 717, 1869, 25920, 328, 17379, 656]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2 = tokenizer2.encode(raw_text)\n",
    "token2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "874563ef-2b5f-4c72-9224-9ca2b4a00ac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice was beginning to get very tired of sitting by'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.decode(token2[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3e4a76-a82c-4123-aba8-df6be624b412",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llmenv)",
   "language": "python",
   "name": "llmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
