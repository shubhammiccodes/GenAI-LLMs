{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c4e9b4-0689-4dd9-895f-ffad94a61e8f",
   "metadata": {},
   "source": [
    "# Embedding using torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f8ee00-d810-41ec-a5bc-60db2ff4a328",
   "metadata": {},
   "source": [
    "### import required pkg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f344f8f-013f-428b-b723-bf318f05677c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e77df581-5335-44b0-8a02-5202fc45d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.txt\", \"r\")  as file:\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53656a12-76d4-4df0-a4b4-99214dc4b5bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Alice was '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8a97184-82b7-4a49-b4d6-b7fee73db9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamLLMDataset(Dataset):\n",
    "    def __init__(self, raw_text, tokenizer, context_length=10, stride=1):      \n",
    "\n",
    "        # create the tokens\n",
    "        tokens = tokenizer.encode(raw_text)\n",
    "\n",
    "        # store the input and target tensors\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # create the input and output tensors \n",
    "        for i in range(0, len(tokens) - context_length, stride):\n",
    "            input_tokens = tokens[i: i + context_length]\n",
    "            target_tokens = tokens[i+1: i + context_length + 1]\n",
    "\n",
    "            self.input_ids.append(torch.tensor(input_tokens))\n",
    "            self.target_ids.append(torch.tensor(target_tokens))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.input_ids[index], self.target_ids[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d862099-2db9-4f2d-b9d8-bae41fd064f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "818285fe-dfa8-4f41-8cb4-c249f20591ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data set\n",
    "dataset = DreamLLMDataset(raw_text = raw_text, tokenizer = tokenizer)\n",
    "\n",
    "# create the data loader\n",
    "data_loader = DataLoader(dataset, batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0afbf050-1a9d-4e98-99d3-5748d3fdccb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the iterator\n",
    "data_loader_iterator = iter(data_loader)\n",
    "\n",
    "# get the first batch\n",
    "inputs, targets = next(data_loader_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "345b8570-c54e-4eff-a398-58ef029c29c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[44484,   373,  3726,   284,   651,   845, 10032,   286,  5586,   416],\n",
       "         [  373,  3726,   284,   651,   845, 10032,   286,  5586,   416,   607],\n",
       "         [ 3726,   284,   651,   845, 10032,   286,  5586,   416,   607,  6621],\n",
       "         [  284,   651,   845, 10032,   286,  5586,   416,   607,  6621,   198],\n",
       "         [  651,   845, 10032,   286,  5586,   416,   607,  6621,   198,   261]]),\n",
       " tensor([[  373,  3726,   284,   651,   845, 10032,   286,  5586,   416,   607],\n",
       "         [ 3726,   284,   651,   845, 10032,   286,  5586,   416,   607,  6621],\n",
       "         [  284,   651,   845, 10032,   286,  5586,   416,   607,  6621,   198],\n",
       "         [  651,   845, 10032,   286,  5586,   416,   607,  6621,   198,   261],\n",
       "         [  845, 10032,   286,  5586,   416,   607,  6621,   198,   261,   262]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3441d599-7374-485c-b926-0ef36ad92738",
   "metadata": {},
   "source": [
    "## create an embedding layer using pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5fd1aa30-59f2-4df2-9b07-6b65c080a9be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the vocab_size\n",
    "vocab_size = tokenizer.n_vocab\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e1f1a65-dc25-4ee6-8b5e-ab4561075010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare the embedding dimension\n",
    "dimensions = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d03f2c79-41c7-47de-8be2-937d9082828e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-1.6524,  0.5827,  0.2203,  0.0866,  0.4616],\n",
       "        [-0.3565, -0.8197, -0.0359, -0.5595,  0.9661],\n",
       "        [ 1.0778,  2.2309,  1.0436, -0.1034, -1.4463],\n",
       "        ...,\n",
       "        [-0.0164,  0.0498, -1.4630, -0.2073,  0.2761],\n",
       "        [-1.1461, -0.1843,  0.9202,  0.0232, -0.4972],\n",
       "        [ 0.4729, -1.7133,  0.0961, -0.5926,  1.3687]], requires_grad=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the embedding layer \n",
    "embedding_layer = torch.nn.Embedding(vocab_size, dimensions)\n",
    "embedding_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "59c45bd7-00b4-4fcc-be7b-6051497205ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.8229, -0.3904,  0.1670, -1.6235,  1.2489],\n",
       "         [ 1.1301, -0.5418,  1.0714,  1.6421, -0.1737],\n",
       "         [-1.1841,  0.1286,  1.0626,  0.9894,  0.0925],\n",
       "         [-0.4297,  0.1926, -0.1690,  0.6104,  0.0106],\n",
       "         [-2.1916,  1.2459, -0.9955, -0.7046,  0.2778],\n",
       "         [-2.6952, -0.4334,  0.0495, -0.0684,  0.7257],\n",
       "         [-0.1990, -1.0723,  0.9373, -0.1661,  0.2087],\n",
       "         [ 0.4891,  0.6249, -0.9196,  1.1644, -0.5797],\n",
       "         [ 0.5255,  0.6536,  2.3863, -1.9749,  0.4202],\n",
       "         [ 0.8682, -1.4933, -0.3441, -0.0957, -0.5528]],\n",
       "\n",
       "        [[ 1.1301, -0.5418,  1.0714,  1.6421, -0.1737],\n",
       "         [-1.1841,  0.1286,  1.0626,  0.9894,  0.0925],\n",
       "         [-0.4297,  0.1926, -0.1690,  0.6104,  0.0106],\n",
       "         [-2.1916,  1.2459, -0.9955, -0.7046,  0.2778],\n",
       "         [-2.6952, -0.4334,  0.0495, -0.0684,  0.7257],\n",
       "         [-0.1990, -1.0723,  0.9373, -0.1661,  0.2087],\n",
       "         [ 0.4891,  0.6249, -0.9196,  1.1644, -0.5797],\n",
       "         [ 0.5255,  0.6536,  2.3863, -1.9749,  0.4202],\n",
       "         [ 0.8682, -1.4933, -0.3441, -0.0957, -0.5528],\n",
       "         [ 0.3290,  2.0280, -1.2761, -0.1451,  0.1800]],\n",
       "\n",
       "        [[-1.1841,  0.1286,  1.0626,  0.9894,  0.0925],\n",
       "         [-0.4297,  0.1926, -0.1690,  0.6104,  0.0106],\n",
       "         [-2.1916,  1.2459, -0.9955, -0.7046,  0.2778],\n",
       "         [-2.6952, -0.4334,  0.0495, -0.0684,  0.7257],\n",
       "         [-0.1990, -1.0723,  0.9373, -0.1661,  0.2087],\n",
       "         [ 0.4891,  0.6249, -0.9196,  1.1644, -0.5797],\n",
       "         [ 0.5255,  0.6536,  2.3863, -1.9749,  0.4202],\n",
       "         [ 0.8682, -1.4933, -0.3441, -0.0957, -0.5528],\n",
       "         [ 0.3290,  2.0280, -1.2761, -0.1451,  0.1800],\n",
       "         [ 1.0302,  1.5578,  1.8063,  0.1410,  0.1538]],\n",
       "\n",
       "        [[-0.4297,  0.1926, -0.1690,  0.6104,  0.0106],\n",
       "         [-2.1916,  1.2459, -0.9955, -0.7046,  0.2778],\n",
       "         [-2.6952, -0.4334,  0.0495, -0.0684,  0.7257],\n",
       "         [-0.1990, -1.0723,  0.9373, -0.1661,  0.2087],\n",
       "         [ 0.4891,  0.6249, -0.9196,  1.1644, -0.5797],\n",
       "         [ 0.5255,  0.6536,  2.3863, -1.9749,  0.4202],\n",
       "         [ 0.8682, -1.4933, -0.3441, -0.0957, -0.5528],\n",
       "         [ 0.3290,  2.0280, -1.2761, -0.1451,  0.1800],\n",
       "         [ 1.0302,  1.5578,  1.8063,  0.1410,  0.1538],\n",
       "         [-0.9298, -0.0283,  0.6946,  0.5591, -0.9445]],\n",
       "\n",
       "        [[-2.1916,  1.2459, -0.9955, -0.7046,  0.2778],\n",
       "         [-2.6952, -0.4334,  0.0495, -0.0684,  0.7257],\n",
       "         [-0.1990, -1.0723,  0.9373, -0.1661,  0.2087],\n",
       "         [ 0.4891,  0.6249, -0.9196,  1.1644, -0.5797],\n",
       "         [ 0.5255,  0.6536,  2.3863, -1.9749,  0.4202],\n",
       "         [ 0.8682, -1.4933, -0.3441, -0.0957, -0.5528],\n",
       "         [ 0.3290,  2.0280, -1.2761, -0.1451,  0.1800],\n",
       "         [ 1.0302,  1.5578,  1.8063,  0.1410,  0.1538],\n",
       "         [-0.9298, -0.0283,  0.6946,  0.5591, -0.9445],\n",
       "         [-0.0804, -0.1400,  0.5268, -0.0985, -0.1050]]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the embeddings of input\n",
    "input_embeddings = embedding_layer(inputs)\n",
    "input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57ba29db-218c-4498-b4a3-9f0c56e71c06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (llmenv)",
   "language": "python",
   "name": "llmenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
